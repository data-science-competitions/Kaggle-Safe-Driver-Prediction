The Gini Coefficient under Binary Classification Problems
========================================================
author: Harel Lustiger
date: October, 2017
autosize: true
transition: none
css: custom.css


```{r get_the_data, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE, results=FALSE}
lapply(c("ROCR","readr"), library, character.only = TRUE)
pusha = getwd()
setwd("C:/Dropbox/Research/Kaggle-Safe-Driver-Prediction/notebooks")
slug = file.path(getwd(),"data")

file_path = file.path(slug,"(results)(same_obj_func_diff_metric).csv")
criteria = c("Gini","Accuracy","AUC","Precision","RMSE")
scores1 = as.data.frame(read_csv(file_path, col_names=criteria, skip=1))

file_path = file.path(slug,"(results)(diff_obj_func_diff_metric).csv")
criteria = c("Gini","Accuracy","AUC","Precision","RMSE")
scores2 = as.data.frame(read_csv(file_path, col_names=criteria, skip=1))[,-1]
scores2 = tidyr::gather(scores2, key="metric", value="value",factor_key=TRUE)

setwd(pusha)
```


Introduction
========================================================
type: section


Real World Example
========================================================
id: real_world_example
type: sub-section

>> New Zealand's Jacinda Ardern sets out priorities: climate, `inequality` and 
women.

What should we measure to determine if social inequality was tackled 
successfully by the end of the PM term?

<green>&#x2192; Suggestion: The Gini Coefficient</green>


Graphical representation of Gini:Lorenz curve, Gaerem Hart and Me
========================================================
id: gini_graphical_representation_before
type: sub-section
title: true

<div align="center">
<img src="images/gini_before.png" width=700 height=600>
</div>


Graphical representation of Gini, Gaerem Hart and Me
========================================================
id: gini_graphical_representation_after
type: sub-section
title: true

<div align="center">
<img src="images/gini_after.png" width=700 height=600>
</div>

<!-- 
NOTES:
* This is a graphical representation of Gini, rather than a scalar
* The y-axis is measures a continuous variable
-->


Projecting the 2D space into a scalar metric (a.k.a. Gini Coefficient)
========================================================
id: gini_scalar
type: sub-section
title: true

```{r echo=TRUE, message=FALSE, warning=FALSE}
GiniCoeff <- function(solution, submission){
    
    df = data.frame(solution = solution, submission = submission)
    df = df[order(df$submission, decreasing = TRUE),]
    df$uniform = (1:nrow(df))/nrow(df) # = (1/n, 2/n, ..., 1)
    totalPos = sum(df$solution) # how many time '1' appears in reality?
    
    # This will store the cumulative number of positive 
    # examples found (used for computing "Model Lorentz")
    df$cumPosFound = cumsum(df$solution) 
    
    # This will store the cumulative proportion of positive examples
    # found ("Model Lorentz")
    df$Lorentz = df$cumPosFound / totalPos 
    
    # This will store Lorentz minus uniform
    df$Gini = df$Lorentz - df$uniform   
    
    return(sum(df$Gini))
}
```

To learn more about "Estimation of the Gini coefficient" see 
[this link](http://www.vcharite.univ-mrs.fr/PP/lubrano/cours/Lecture-4.pdf)
(p. 14)


Normalized Gini Coefficient Attributes
========================================================
id: norm_gini_scalar
type: sub-section
title: true

```{r echo=TRUE, message=FALSE, warning=FALSE}
NormGiniCoeff <- function(solution, submission){
    
    GiniCoeff(solution, submission) / GiniCoeff(solution, solution)
    
}
```

$$
0\leq \text{gini} \leq 1
$$


Context
========================================================
id: context
type: sub-section

**Featured Prediction Competition:**   
[Porto Seguro's Safe Driver Prediction](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction).

**Problem:**  
Predict if a driver will file an insurance claim next year.

**Objective:**  
Maximize the (Normalized) Gini Coefficient. 

**Prediction Type**  
Classificaton with emphasize on *scoring classifiers*.

<!-- 
NOTES:

One observation worth making in the current competition is that the classifier 
scores need to be only relative.

Neither us nor the competition's organizers are going to threshold the scores
into a confusion matrix and derive from it the winning submission.
-->

Scoring Classifiers
========================================================
type: section

What is Scoring?
========================================================
id: what_is_scoring
type: sub-section

```{r, echo=FALSE}
n = 6 # for simplicity, choose an even number
```

Given `r n` members from the insurance company database and their *true future 
claim result*, we obserbe two algorithms outputs:  

```{r scoring_example_1, echo=FALSE, results='asis'}
set.seed(1547)

y = rep(c(0,1),each=n/2)
member_result = factor(y, labels=c("No","Yes"))
member_name = toupper(letters[1:n])

algorithm_score_1 = 1:n
algorithm_score_2 = c(sample(1:(n/2), n/2),
                      sample((1+n/2):n, n/2))
example = data.frame("Member Name"=member_name,
                     "Ground Truth"=member_result,
                     "1st Algorithm Score"=algorithm_score_1,
                     "2nd Algorithm Score"=algorithm_score_2,
                     check.names=F)
knitr::kable(example, format="markdown", align="c")
```

Are the two algorithms' **scores** the same? **YES**, both would yield the same Gini 
coeff value

Are the two algorithms' **ranks** the same? **NO**, but who cares?!

<!--
Note that utilizing ranking algorithms can be a substitude for scoring 
algorithms, but the opposite does not necessarily hold.
-->


Scoring: Takeaways
========================================================
id: scoring_the_takeaways
type: sub-section

* The algorithms need <red>not</red> rank all the instances in a desired order. 
What is important is that the positive instances are generally ranked higher
than the negative ones.
* The scores need <red>not</red> be in any predefined intervals, nor be 
likelihoods or probabilities over class memberships.
* The overall hope is that the classifier typically scores the positive examples
higher than the negative examples.

<green>&#x2192; The door for regression algorithms and their objective functions
is open.<green/>


Key Attributes of the Target Variable
========================================================
id: key_attributes_of_y
type: alert

<!-- prompt / alert -->

* The target variable has two states, '0' and '1'.  
* The target variable is skewed, while we are eager to find the rare class.

```{r, echo=FALSE, results='as.is'}
tab = data.frame("0"=573518, "1"=21694, check.names=FALSE)
tab[2,1] = round(100*573518/(573518+21694))
tab[2,2] = 100-tab[2,1]
rownames(tab) = c("Frequency","Proportion")
knitr::kable(tab, format="markdown", align="c", row.names=TRUE)
```

**Why is it important?**

Supervised learning rests on the assumption that the initial training set is 
going to produce a useful model for discriminating between classes.

Furthermore, in their basic form, most classifiers do not behave well on 
unbalanced data sets. Instead, most classifiers have predictive preference for 
the class with the greater proportion of examples.


Gini and Other Performance Measures in Practice
========================================================
id: gini_and_friends
type: section


Experimental Setup
========================================================
id: experimental_setup_1
type: sub-section

* **Data Spliting** - The train set is split into 4 unequal parts:
* *(10%) training set* - <green>used for fitting `xgboost` models</green>.
* *(10%) evaluation set* - given a <blue>performance measure</blue> (such as 
AUC), the model performance is evaluated based on this set. 
If there is no improvment for $10$ rounds, the training stops.
* *(10%) test set* - used as unseen data for asssing the differen models.
* *(70%) validation set* - <red>not in use at all</red>.


Experimental Setup
========================================================
id: experimental_setup_2
type: sub-section

* **Data Perprocessing**
* Remove all rows containing missing values.
* Remove all cols which are not continous variables (inc. order factor).
* <green>&#x2192; train, eval and test sets all have 12 variables and around 
40,000 rows</green>
* **Model Fitting**
* For each <blue>performance measure</blue>, there are 200 bootstrap 
`xgboost` models.
* Before each run, the train set is sampled with replacment (i.e. bootstrap 
sample).
* The other sets (eval and test) remain unchanged.


Experimental Setup
========================================================
id: experimental_setup_3
type: sub-section

* **Parameter Tuning** - The parameters were chosen in accordance with Anton 
Aksyonov suggestions which can be 
[found here](https://www.kaggle.com/antonaks/first-steps-with-xgboost-model-0-27/code).

```{r eval=FALSE, include=TRUE}
model <- xgb.train(data=dX_bs,
                   # Parameter for Tree Booster
                   max_depth=6,
                   eta=0.02,
                   gamma=1,
                   subsample=0.95,
                   colsample_bytree=0.8,
                   min_child_weight=20,
                   # Early Stopping to Avoid Overfitting
                   nrounds=1000,
                   early_stopping_rounds=10,
                   watchlist=list(train=dX_bs, test=dX_ev),
                   # Task Parameters
                   objective="binary:logistic",
                   eval_metric=eval_metric, 
                   # eval_metric \in {"error","auc","map","rmse"}
                   seed=2145)
```


1st Experiment
========================================================
id: experiment_1
type: sub-section

**Correlation Plots** between `Gini` and selected 
<blue>performance measure</blue>

```{r correlation_plots, echo=FALSE, fig.align='center', fig.width=9, fig.height=4, dpi=300}
par(pty="s", mfrow=c(1,4), mar=c(2, 0.5, 2.5, 0.2), oma=c(4, 4, 0.2, 0.2))
for(criterion in c("Accuracy","AUC","Precision","RMSE")){
    # 1. Build the plot structure
    plot(scores1[,criterion], scores1[,"Gini"], type="n",
         ylab="Gini", xlab=criterion,
         cex.lab=1.2, cex.axis=1.2, cex.main=1.2,
         main=paste0("Gini as a function of ",criterion))
    # 2. Plot the data points
    points(scores1[,criterion], scores1[,"Gini"], pch=16, col="deepskyblue")
    # 3. The regression line
    abline(lm(scores1[,"Gini"]~scores1[,criterion]), col="red", lwd=2) # regression line (y~x)
    text(mean(range(scores1[,criterion])),
         mean(range(scores1[,"Gini"])),
         paste("r =",with(scores1,round(cor(scores1[,criterion],Gini),2))),
         col="red", cex=2, font=2) 
}
```

$$
\text{AUC}=(\text{Gini}+1)/2 
\quad\leftrightarrow\quad
\text{Gini}=2\times \text{AUC}-1
$$


2nd Experiment
========================================================
id: experiment_2
type: sub-section

**Boxplots** of selected <blue>performance measure</blue> which were used to  
stop `xgboost` training

```{r boxplots, echo=FALSE, fig.align='center', fig.width=12, fig.height=6}
par(pty="m", mfrow=c(1,1), mar=c(0, 5, 2, 0), oma=c(4, 4, 0.2, 0.2))
boxplot(value~metric, data=scores2,
        ylab="Gini",
        cex.lab=1.8, cex.axis=1.8, cex.names=1.5)
```

