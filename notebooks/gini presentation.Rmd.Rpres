The Gini Coefficient under Binary Classification Problems
========================================================
author: Harel Lustiger
date: October, 2017
autosize: true
transition: none
css: custom.css


Introduction
========================================================
type: section


Real World Example
========================================================
id: real_world_example
type: sub-section


>> New Zealand's Jacinda Ardern sets out priorities: climate, `inequality` and 
women.

What should we measure to determine if social inequality was tackled 
successfully by the end of the PM term?

<green>&#x2192; Suggestion: The Gini Coefficient</green>


Graphical representation of Gini, Gaerem Hart and Me
========================================================
type: sub-section
title: false

![alt text](images/gini_graphical_representation.png)

<!-- 
NOTES:
* This is a graphical representation of Gini, rather than a scalar
* The y-axis is measures a continuous variable
-->

Objective and Key Points
========================================================
type: section

Main Points
========================================================
type: sub-section

1. What are the relationships between Gini and and other criteria?
2. How does Gini behaive under unbalanced target variable?
3. How can we elicit more infromation from Gini?

Context
========================================================
type: sub-section

**Featured Prediction Competition:**   
[Porto Seguroâ€™s Safe Driver Prediction](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction).

**Problem:**  
Predict if a driver will file an insurance claim next year.

**Objective:**  
Maximize the (Normalized) Gini Coefficient. 

**Prediction Type**  
Classificaton with emphasize on *scoring classifiers*.

<!-- 
NOTES:

One observation worth making in the current competition is that the classifier 
scores need to be only relative.

Neither us nor the competition's organizers are going to threshold the scores
into a confusion matrix and derive from it the winning submission.
-->

Scoring Classifiers
========================================================
type: section

What is Scoring?
========================================================
id: what_is_scoring
type: sub-section

```{r, echo=FALSE}
n = 6 # for simplicity, choose an even number
```

Given `r n` members from the insurance company database and their *true future 
claim result*, we obserbe two algorithms outputs:  

```{r scoring_example, echo=FALSE, results='asis'}
set.seed(1547)
member_name = toupper(letters[1:n])
member_result = gl(2,round(n/2), labels=c("No","Yes"))
algorithm_score_1 = 1:n
algorithm_score_2 = c(sample(1:(n/2), n/2),
                      sample((1+n/2):n, n/2))
example = data.frame("Member Name"=member_name,
                     "Ground Truth"=member_result,
                     "1st Algorithm Score"=algorithm_score_1,
                     "2nd Algorithm Score"=algorithm_score_2,
                     check.names=F)
knitr::kable(example, format="markdown", align="c")
```

Are the two algorithms' **scores** the same? **YES**, both would yield the same Gini 
coeff value

Are the two algorithms' **ranks** the same? **NO**, but who cares?!

<!--
Note that utilizing ranking algorithms can be a substitude for scoring 
algorithms, but the opposite does not necessarily hold.
-->


Scoring: Takeaways
========================================================
id: scoring_the_takeaways
type: sub-section

* The algorithms need <red>not</red> rank all the instances in a desired order. 
What is important is that the positive instances are generally ranked higher
than the negative ones.
* The scores need <red>not</red> be in any predefined intervals, nor be 
likelihoods or probabilities over class memberships.
* The overall hope is that the classifier typically scores the positive examples
higher than the negative examples.

<green>&#x2192; The door for regression algorithms and their objective functions
is open.<green/>


Key Attributes of the Target Variable
========================================================
id: key_attributes_of_y
type: alert

<!-- prompt / alert -->

* The target variable has two states, '0' and '1'.  
* The target variable is skewed, while we are eager to find the rare class.

```{r, echo=FALSE, results='as.is'}
tab = data.frame("0"=573518, "1"=21694, check.names=FALSE)
tab[2,1] = round(100*573518/(573518+21694))
tab[2,2] = 100-tab[2,1]
rownames(tab) = c("Frequency","Proportion")
knitr::kable(tab, format="markdown", align="c", row.names=TRUE)
```

**Why is it important?**

Supervised learning rests on the assumption that the initial training set is 
going to produce a useful model for discriminating between classes.

Furthermore, in their basic form, most classifiers do not behave well on 
unbalanced data sets. Instead, most classifiers have predictive preference for 
the class with the greater proportion of examples.


Gini and Other Performance Measures in Practice
========================================================
id: gini_and_friends
type: section


Experimental Setup
========================================================
id: experimental_setup_1
type: sub-section

* **Data Spliting** - The train set is split into 4 unequal parts:
    * *(10%) training set* - <green>used for fitting `xgboost` models</green>.
    * *(10%) evaluation set* - given a <blue>performance measure</blue> (such as 
        AUC), the model performance is evaluated based on this set. 
        If there is no improvment for $10$ rounds, the training stops.
    * *(10%) test set* - used as unseen data for asssing the differen models.
    * *(70%) validation set* - <red>not in use at all</red>.


Experimental Setup
========================================================
id: experimental_setup_2
type: sub-section

* **Data Perprocessing**
    * Remove all rows containing missing values.
    * Remove all cols which are not continous variables (inc. order factor).
    * <green>&#x2192; train, eval and test sets all have 12 variables and around 
    40,000 rows</green>
* **Model Fitting**
    * For each <blue>performance measure</blue>, there are 200 bootstrap models.
    * Before each run, the train set is sampled with replacment (i.e. bootstrap 
    sample).
    * The other sets (eval and test) remain unchanged.
    
Experiments and Results
========================================================
id: experiments_and_Results
type: section   

```{r get_the_data, include=FALSE}
lapply(c("ROCR","readr"), library, character.only = TRUE)
setwd('..')

file_path = file.path(getwd(),"pred","(results)(same_obj_func_diff_metric).csv")
criteria = c("Gini","Accuracy","Adjusted Accuracy","AUC","Precision","RMSE")
scores1 = as.data.frame(read_csv(file_path, col_names=criteria, skip=1))

file_path = file.path(getwd(),"pred","(results)(diff_obj_func_diff_metric).csv")
criteria = c("Gini","Accuracy","Adjusted Accuracy","AUC","Precision","RMSE")
scores2 = as.data.frame(read_csv(file_path, col_names=criteria, skip=1))[,-1]
scores2 = tidyr::gather(scores2, key="metric", value="value",factor_key=TRUE)
```


1st Experiment
========================================================
id: experiment_1
type: sub-section

**Correlation Plots** between `Gini` and selected 
<blue>performance measure</blue>

```{r correlation_plots, echo=FALSE, fig.align='center', fig.width=9, fig.height=4, dpi=300}
par(pty="s", mfrow=c(1,4), mar=c(2, 0.5, 2.5, 0.2), oma=c(4, 4, 0.2, 0.2))
for(criterion in c("Accuracy","AUC","Precision","RMSE")){
    # 1. Build the plot structure
    plot(scores1[,criterion], scores1[,"Gini"], type="n",
         ylab="Gini", xlab=criterion,
         cex.lab=1.2, cex.axis=1.2, cex.main=1.2,
         main=paste0("Gini as a function of ",criterion))
    # 2. Plot the data points
    points(scores1[,criterion], scores1[,"Gini"], pch=16, col="deepskyblue")
    # 3. The regression line
    abline(lm(scores1[,"Gini"]~scores1[,criterion]), col="red", lwd=2) # regression line (y~x)
    text(mean(range(scores1[,criterion])),
         mean(range(scores1[,"Gini"])),
         paste("r =",with(scores1,round(cor(scores1[,criterion],Gini),2))),
         col="red", cex=2, font=2) 
}
dev.off()
```

$$
\text{AUC}=(\text{Gini}+1)/2 
\quad\leftrightarrow\quad
\text{Gini}=2\times \text{AUC}-1
$$


2nd Experiment
========================================================
id: experiment_2
type: sub-section

**Boxplots** of selected <blue>performance measure</blue> which were used to  
stop the model training

```{r boxplots, echo=FALSE, fig.align='center', fig.width=12, fig.height=7}
boxplot(value~metric, data=scores2,
        ylab="Gini",
        cex.lab=1.5, cex.axis=1.5, cex.names=1.5)
```

